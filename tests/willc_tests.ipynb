{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 072520 Cumber tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cumberify(f):\n",
    "    img = cv2.imdecode(np.frombuffer(f.read(), np.uint8), 1)\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV) # convert to hsv colorspace because we get better accuracy?\n",
    "    lower_green = np.array([25,50,50])\n",
    "    upper_green = np.array([80,255,255]) # took too damn long to find these values\n",
    "    mask = cv2.inRange(hsv, lower_green, upper_green) # create mask for all greens and yellows\n",
    "    mask = mask/255\n",
    "    mask = mask.astype(np.bool)\n",
    "    \n",
    "    cumbered = np.argwhere(mask) # get idxs of green pixels\n",
    "    \n",
    "    # get rectangle coords\n",
    "    start = (cumbered[0][1], cumbered[0][0])\n",
    "    end = (cumbered[round(len(cumbered)*0.5)][1], cumbered[round(len(cumbered)*0.5)][0])\n",
    "    width = end[0] - start[0]\n",
    "    length = end[1] - start[1]\n",
    "    \n",
    "    # draw a rectangle around part of the cucumber (20% looks too small in most cases)\n",
    "    cv2.rectangle(img, start, end, (0,0,0), -1)\n",
    "    \n",
    "    # get censored coords\n",
    "    C_OFF = 0.8\n",
    "    censored_start = (start[0] + int((width-width*C_OFF)/2), start[1] + int((length-length*C_OFF)/2))\n",
    "    \n",
    "    # put censored image on\n",
    "    img_PIL = Image.fromarray(img)\n",
    "    img_PIL.show()\n",
    "    censored = Image.open('../images/censored.png')\n",
    "    img_PIL.paste(censored.resize((int(width*C_OFF),int(length*C_OFF))), censored_start)\n",
    "#     img_PIL.show()\n",
    "    \n",
    "#     img = cv2.addWeighted(img, 0.4, censored, 0.1, 0)\n",
    "\n",
    "#     _, buffer = cv2.imencode(\".jpg\", img)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f = open('test2.jpeg', 'rb')\n",
    "modified_cumber = _cumberify(f)\n",
    "\n",
    "img = Image.fromarray(modified_cumber)\n",
    "# img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 1, 2)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1,2)*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 072620 Verbosify Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: you think we will achieve this win, brothers?\n",
      "verbosified: you conceive we volition accomplish this winnings, buddy?\n"
     ]
    }
   ],
   "source": [
    "input_sentence = 'you think we will achieve this win, brothers?'\n",
    "new_sentence = ''\n",
    "\n",
    "# go through every word in sentence\n",
    "for word in re.findall(r\"\\w+|[^\\w\\s]\", input_sentence):\n",
    "    # punctuation\n",
    "    if re.match(r\"[^\\w\\s]\", word):\n",
    "        new_sentence += word\n",
    "        continue\n",
    "    \n",
    "    # look for synonym until we find unique one\n",
    "    while True:\n",
    "        synsets = wordnet.synsets(word)\n",
    "\n",
    "        # no synonyms\n",
    "        if not synsets:\n",
    "            new_sentence += ' ' + word\n",
    "            break\n",
    "            \n",
    "        # choose random synonym for random synset\n",
    "        synonym = random.choice(random.choice(synsets).lemmas()).name()\n",
    "        if synonym.lower() not in word.lower():\n",
    "            new_sentence += ' ' + synonym\n",
    "            break\n",
    "\n",
    "\n",
    "print('original:', input_sentence)\n",
    "print('verbosified:', new_sentence[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of Speech exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "# or run: `nltk.help.upenn_tagset()`\n",
    "all_tags = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS',\n",
    "            'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH',\n",
    "            'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n",
    "\n",
    "# https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "# https://linguistics.stackexchange.com/questions/6508/which-part-of-speech-are-s-and-r-in-wordnet\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'): return 'as'\n",
    "    elif treebank_tag.startswith('V'): return 'v'\n",
    "    elif treebank_tag.startswith('N'): return 'n'\n",
    "    elif treebank_tag.startswith('R'): return 'r'\n",
    "    else: return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_syns(word):\n",
    "    if not wordnet.synsets(word):\n",
    "        print('no synonyms found')\n",
    "        return\n",
    "\n",
    "    for syn in wordnet.synsets(word):\n",
    "        print(syn.name() + ':')\n",
    "        for lemma in syn.lemmas():\n",
    "            print(lemma)\n",
    "\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parts of speech: [('hop', 'NN'), ('in', 'IN'), ('the', 'DT'), ('voice', 'NN'), ('channel', 'NN'), ('and', 'CC'), ('find', 'VB'), ('out', 'RP'), ('for', 'IN'), ('yourself', 'PRP')]\n",
      "\n",
      "parts of speech: [('hop', 'NN'), ('in', 'IN'), ('the', 'DT'), ('voice', 'NN'), ('channel', 'NN'), ('and', 'CC'), ('find_out', 'NN'), ('for', 'IN'), ('yourself', 'NN'), ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentence = re.findall(r\"\\w+|[^\\w\\s]\", 'hop in the voice channel and find out for yourself')\n",
    "print('parts of speech:', nltk.pos_tag(tokenized_sentence))\n",
    "\n",
    "print()\n",
    "\n",
    "tokenized_sentence = ['hop', 'in', 'the', 'voice', 'channel', 'and', 'find_out', 'for', 'yourself', '!']\n",
    "print('parts of speech:', nltk.pos_tag(tokenized_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('ugly.a.01.ugly')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synset('beautiful.a.01').lemmas()[0].antonyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test parts of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitelist = {'a/DT': ['an', 'the'],\n",
    "             'an/DT': ['a', 'the'],\n",
    "             'the/DT': ['a', 'an'],\n",
    "             'I/PRP': ['ur boy', 'me, myself and I', 'yours truly'],\n",
    "             'me/PRP': 'I/PRP',\n",
    "             'you/PRP': ['thou', 'thoust'],\n",
    "             'will/MD': ['shall', 'shalt']}\n",
    "\n",
    "\n",
    "def verbosify2(input_sentence, num_tries = 30):\n",
    "    word_list = []\n",
    "\n",
    "    # go through every word    \n",
    "    for word, pos in pos_tag(re.findall(r\"\\w+|[^\\w\\s]\", input_sentence)):\n",
    "        # punctuation, whitelist, or normal word\n",
    "        if re.match(r\"[^\\w\\s]\", word): word_list.append(word)\n",
    "        elif word+'/'+pos in whitelist: word_list.append(get_whitelist_synonym(word, pos))\n",
    "        else: word_list.append(get_synonym(word, get_wordnet_pos(pos)))\n",
    "\n",
    "    return join_sentence(word_list)\n",
    "\n",
    "\n",
    "def get_synonym(word, pos):\n",
    "    synsets = wordnet.synsets(word)\n",
    "    synonyms = []\n",
    "\n",
    "    # loop through all synsets\n",
    "    for synset in synsets:\n",
    "        # don't check synset if wrong part of speech\n",
    "        if synset.name().split('.')[1] not in pos: continue\n",
    "\n",
    "        # loop through each synonym\n",
    "        for synonym in synset.lemmas():\n",
    "            synonym = synonym.name()\n",
    "            if synonym != word and synonym not in synonyms: synonyms.append(synonym)\n",
    "    \n",
    "    # no unique synonyms?\n",
    "    if not synsets or not synonyms: return word\n",
    "    # otherwise, choose random synonym\n",
    "    return random.choice(synonyms)\n",
    "\n",
    "def get_whitelist_synonym(word, pos):\n",
    "    synonyms = whitelist[word+'/'+pos]\n",
    "    if isinstance(synonyms, list): return random.choice(synonyms + [word])\n",
    "    else: return random.choice(whitelist[synonyms] + [word]) # reference to another entry\n",
    "    \n",
    "\n",
    "def join_sentence(word_list):\n",
    "    new_sentence = ''\n",
    "    \n",
    "    for word in word_list:\n",
    "        if re.match(r\"[^\\w\\s]\", word): new_sentence += word\n",
    "        else: new_sentence += ' ' + word.replace('_', ' ')\n",
    "            \n",
    "    return new_sentence[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me, myself and I would care the ground beef misrepresent precisely for yours truly\n",
      "I would care a burger wangle but for me\n",
      "ur boy would wish an beefburger ready precisely for me, myself and I\n",
      "I would care the ground beef falsify but for ur boy\n",
      "me, myself and I would care a burger fix barely for me, myself and I\n",
      "me, myself and I would care the ground beef cook hardly for ur boy\n",
      "yours truly would care a ground beef make scarcely for ur boy\n",
      "yours truly would care the ground beef fudge exactly for me\n",
      "yours truly would care an burger misrepresent simply for ur boy\n",
      "ur boy would wish an ground beef fudge precisely for yours truly\n",
      "yours truly would care an beefburger falsify simply for me\n",
      "yours truly would care a beefburger manipulate just now for ur boy\n",
      "me, myself and I would care a ground beef wangle just now for me\n",
      "ur boy would care the beefburger fix merely for me\n",
      "I would wish an beefburger ready hardly for ur boy\n",
      "I would care an beefburger cook merely for me, myself and I\n",
      "ur boy would care the ground beef ready merely for ur boy\n",
      "I would wish an burger fudge merely for me, myself and I\n",
      "me, myself and I would wish an ground beef cook precisely for me\n",
      "I would care the beefburger cook hardly for yours truly\n"
     ]
    }
   ],
   "source": [
    "sim = 20\n",
    "\n",
    "input_sentence = 'I would like a hamburger cooked just for me'\n",
    "for i in range(sim):\n",
    "    print(verbosify2(input_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/william/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = 'I would like a hamburger cooked just for me'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I would like a hamburger cooked just for me'"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
